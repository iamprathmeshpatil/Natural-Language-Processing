{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:19:35.143005Z",
     "start_time": "2017-10-27T02:19:34.009498Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import re\n",
    "import collections\n",
    "import itertools\n",
    "import bcolz\n",
    "import pickle\n",
    "sys.path.append('../../lib')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import random\n",
    "import smart_open\n",
    "import h5py\n",
    "import csv\n",
    "import json\n",
    "import functools\n",
    "import time\n",
    "import string\n",
    "\n",
    "import datetime as dt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import global_utils\n",
    "\n",
    "random_state_number = 967898"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:19:35.593636Z",
     "start_time": "2017-10-27T02:19:35.144271Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/gpu:0', '/gpu:1']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:19:35.677383Z",
     "start_time": "2017-10-27T02:19:35.594851Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bicepjai/Programs/anaconda3/envs/dsotc-c3/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['random']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab\n",
    "%matplotlib inline\n",
    "%load_ext line_profiler\n",
    "%load_ext memory_profiler\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:19:35.681672Z",
     "start_time": "2017-10-27T02:19:35.678670Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "color = sns.color_palette()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:19:52.907193Z",
     "start_time": "2017-10-27T02:19:35.683357Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store = pd.HDFStore('../../data_prep/processed/stage1/data_frames.h5')\n",
    "train_df = store['train_df']\n",
    "test_df = store['test_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:19:53.225229Z",
     "start_time": "2017-10-27T02:19:52.908346Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variation</th>\n",
       "      <th>Class</th>\n",
       "      <th>Sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[fam58a]</td>\n",
       "      <td>[truncating, mutations]</td>\n",
       "      <td>1</td>\n",
       "      <td>[[cyclin-dependent, kinases, , cdks, , regulat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[cbl]</td>\n",
       "      <td>[w802*]</td>\n",
       "      <td>2</td>\n",
       "      <td>[[abstract, background, non-small, cell, lung,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[cbl]</td>\n",
       "      <td>[q249e]</td>\n",
       "      <td>2</td>\n",
       "      <td>[[abstract, background, non-small, cell, lung,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[cbl]</td>\n",
       "      <td>[n454d]</td>\n",
       "      <td>3</td>\n",
       "      <td>[[recent, evidence, has, demonstrated, that, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[cbl]</td>\n",
       "      <td>[l399v]</td>\n",
       "      <td>4</td>\n",
       "      <td>[[oncogenic, mutations, in, the, monomeric, ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID      Gene                Variation  Class  \\\n",
       "0   0  [fam58a]  [truncating, mutations]      1   \n",
       "1   1     [cbl]                  [w802*]      2   \n",
       "2   2     [cbl]                  [q249e]      2   \n",
       "3   3     [cbl]                  [n454d]      3   \n",
       "4   4     [cbl]                  [l399v]      4   \n",
       "\n",
       "                                           Sentences  \n",
       "0  [[cyclin-dependent, kinases, , cdks, , regulat...  \n",
       "1  [[abstract, background, non-small, cell, lung,...  \n",
       "2  [[abstract, background, non-small, cell, lung,...  \n",
       "3  [[recent, evidence, has, demonstrated, that, a...  \n",
       "4  [[oncogenic, mutations, in, the, monomeric, ca...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variation</th>\n",
       "      <th>Sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[acsl4]</td>\n",
       "      <td>[r570s]</td>\n",
       "      <td>[[2, this, mutation, resulted, in, a, myelopro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[naglu]</td>\n",
       "      <td>[p521l]</td>\n",
       "      <td>[[abstract, the, large, tumor, suppressor, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[pah]</td>\n",
       "      <td>[l333f]</td>\n",
       "      <td>[[vascular, endothelial, growth, factor, recep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[ing1]</td>\n",
       "      <td>[a148d]</td>\n",
       "      <td>[[inflammatory, myofibroblastic, tumor, , imt,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[tmem216]</td>\n",
       "      <td>[g77a]</td>\n",
       "      <td>[[abstract, retinoblastoma, is, a, pediatric, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID       Gene Variation                                          Sentences\n",
       "0   0    [acsl4]   [r570s]  [[2, this, mutation, resulted, in, a, myelopro...\n",
       "1   1    [naglu]   [p521l]  [[abstract, the, large, tumor, suppressor, 1, ...\n",
       "2   2      [pah]   [l333f]  [[vascular, endothelial, growth, factor, recep...\n",
       "3   3     [ing1]   [a148d]  [[inflammatory, myofibroblastic, tumor, , imt,...\n",
       "4   4  [tmem216]    [g77a]  [[abstract, retinoblastoma, is, a, pediatric, ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_df.head())\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:19:53.324444Z",
     "start_time": "2017-10-27T02:19:53.226323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352220 352220\n"
     ]
    }
   ],
   "source": [
    "corpus_vocab_list, corpus_vocab_wordidx = None, None\n",
    "with open('../../data_prep/processed/stage1/vocab_words_wordidx.pkl', 'rb') as f:\n",
    "    (corpus_vocab_list, corpus_wordidx) = pickle.load(f)\n",
    "print(len(corpus_vocab_list), len(corpus_wordidx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-14T08:20:17.449244Z",
     "start_time": "2017-08-14T08:20:15.593136Z"
    },
    "collapsed": true
   },
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To control the vocabulary pass in updated corpus_wordidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:19:53.363624Z",
     "start_time": "2017-10-27T02:19:53.325591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2988, 5)\n",
      "(333, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train_df, x_val_df = train_test_split(train_df,\n",
    "                                         test_size=0.10, random_state=random_state_number,\n",
    "                                         stratify=train_df.Class)\n",
    "\n",
    "print(x_train_df.shape)\n",
    "print(x_val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:20:00.665748Z",
     "start_time": "2017-10-27T02:19:53.364857Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.keras.python.keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:20:00.669391Z",
     "start_time": "2017-10-27T02:20:00.667078Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size=len(corpus_vocab_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## T:sent_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T00:14:20.659301Z",
     "start_time": "2017-10-26T00:14:20.653375Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "custom_unit_dict = {\n",
    "         \"gene_unit\"      : \"words\",\n",
    "         \"variation_unit\" : \"words\",\n",
    "         # text transformed to sentences attribute\n",
    "         \"doc_unit\"       : \"words\",\n",
    "         \"doc_form\"       : \"sentences\",\n",
    "         \"divide_document\": \"multiple_unit\"\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T00:16:51.148007Z",
     "start_time": "2017-10-26T00:14:22.265259Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import global_utils\n",
    "gen_data = global_utils.GenerateDataset(x_train_df, corpus_wordidx)\n",
    "x_train_21_T, x_train_21_G, x_train_21_V, x_train_21_C = gen_data.generate_data(custom_unit_dict, \n",
    "                                                                             has_class=True,\n",
    "                                                                             add_start_end_tag=True)\n",
    "del gen_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T00:16:55.113872Z",
     "start_time": "2017-10-26T00:16:51.149180Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      "(2622081,) [364606, 113692, 197002, 330024, 326252, 151042, 75648, 1818, 276247, 61043, 228115, 326252, 74974, 301275, 76659, 326252, 361104, 329709, 253643, 205596, 153283, 326252, 80594, 326252, 113692, 18820, 349251, 59442, 123801, 228752, 245229, 307200, 17105, 60555, 69032, 1818, 274163, 151942, 246684, 222367, 253643, 243777, 274163, 50915, 274163, 12413, 1818, 228752, 364603, 232434, 214275, 235155, 163151, 123801, 101614, 101366, 364607]\n",
      "(2622081, 3) [364606, 97957, 364607]\n",
      "(2622081,) [364606, 326252, 364607]\n",
      "(2622081,) 6\n"
     ]
    }
   ],
   "source": [
    "print(\"Train data\")\n",
    "print(np.array(x_train_21_T).shape, x_train_21_T[0])\n",
    "print(np.array(x_train_21_G).shape, x_train_21_G[0])\n",
    "print(np.array(x_train_21_V).shape, x_train_21_V[0])\n",
    "print(np.array(x_train_21_C).shape, x_train_21_C[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T00:17:10.385302Z",
     "start_time": "2017-10-26T00:16:55.114982Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gen_data = global_utils.GenerateDataset(x_val_df, corpus_wordidx)\n",
    "x_val_21_T, x_val_21_G, x_val_21_V, x_val_21_C = gen_data.generate_data(custom_unit_dict, \n",
    "                                                                             has_class=True,\n",
    "                                                                             add_start_end_tag=True)\n",
    "del gen_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T00:17:10.853790Z",
     "start_time": "2017-10-26T00:17:10.386619Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val data\n",
      "text (293702,)\n",
      "gene (293702, 3) [364606, 112978, 364607]\n",
      "variation (293702,) [364606, 295010, 364607]\n",
      "classes (293702,) 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Val data\")\n",
    "print(\"text\",np.array(x_val_21_T).shape)\n",
    "print(\"gene\",np.array(x_val_21_G).shape, x_val_21_G[0])\n",
    "print(\"variation\",np.array(x_val_21_V).shape, x_val_21_V[0])\n",
    "print(\"classes\",np.array(x_val_21_C).shape, x_val_21_C[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T00:17:10.857143Z",
     "start_time": "2017-10-26T00:17:10.854958Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_unknown_tag_idx   = corpus_wordidx[\"<UNK>\"]\n",
    "char_unknown_tag_idx   = global_utils.char_unknown_tag_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T00:17:10.872469Z",
     "start_time": "2017-10-26T00:17:10.858162Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MAX_SENT_LEN = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T00:17:27.327704Z",
     "start_time": "2017-10-26T00:17:10.873609Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2622081, 60) (293702, 60)\n"
     ]
    }
   ],
   "source": [
    "x_train_21_T = pad_sequences(x_train_21_T, maxlen=MAX_SENT_LEN, value=word_unknown_tag_idx,\n",
    "                                  padding=\"post\",truncating=\"post\")\n",
    "x_val_21_T = pad_sequences(x_val_21_T, maxlen=MAX_SENT_LEN, value=word_unknown_tag_idx,\n",
    "                                  padding=\"post\",truncating=\"post\")\n",
    "print(x_train_21_T.shape, x_val_21_T.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "keras np_utils.to_categorical expects zero index categorical variables\n",
    "\n",
    "https://github.com/fchollet/keras/issues/570"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T00:17:27.564842Z",
     "start_time": "2017-10-26T00:17:27.328915Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_train_21_C = np.array(x_train_21_C) - 1\n",
    "x_val_21_C = np.array(x_val_21_C) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T00:17:27.631996Z",
     "start_time": "2017-10-26T00:17:27.566066Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2622081, 9) (293702, 9)\n"
     ]
    }
   ],
   "source": [
    "x_train_21_C = np_utils.to_categorical(np.array(x_train_21_C), 9)\n",
    "x_val_21_C = np_utils.to_categorical(np.array(x_val_21_C), 9)\n",
    "print(x_train_21_C.shape, x_val_21_C.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## T:text_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T00:32:43.191953Z",
     "start_time": "2017-10-26T00:32:43.173899Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "custom_unit_dict = {\n",
    "         \"gene_unit\"      : \"words\",\n",
    "         \"variation_unit\" : \"words\",\n",
    "         # text transformed to sentences attribute\n",
    "         \"doc_unit\"       : \"words\",\n",
    "         \"doc_form\"       : \"text\",\n",
    "         \"divide_document\": \"single_unit\"\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T00:32:53.910517Z",
     "start_time": "2017-10-26T00:32:43.193444Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import global_utils\n",
    "gen_data = global_utils.GenerateDataset(x_train_df, corpus_wordidx)\n",
    "x_train_22_T, x_train_22_G, x_train_22_V, x_train_22_C = gen_data.generate_data(custom_unit_dict, \n",
    "                                                                             has_class=True,\n",
    "                                                                             add_start_end_tag=True)\n",
    "del gen_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T00:32:54.978928Z",
     "start_time": "2017-10-26T00:32:53.911691Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      "text (2988,)\n",
      "gene (2988, 3) [352216, 164788, 352217]\n",
      "variation (2988,) [352216, 86196, 352217]\n",
      "classes (2988,) 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Train data\")\n",
    "print(\"text\",np.array(x_train_22_T).shape)\n",
    "print(\"gene\",np.array(x_train_22_G).shape, x_train_22_G[0])\n",
    "print(\"variation\",np.array(x_train_22_V).shape, x_train_22_V[0])\n",
    "print(\"classes\",np.array(x_train_22_C).shape, x_train_22_C[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T00:32:56.258929Z",
     "start_time": "2017-10-26T00:32:54.980078Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gen_data = global_utils.GenerateDataset(x_val_df, corpus_wordidx)\n",
    "x_val_22_T, x_val_22_G, x_val_22_V, x_val_22_C = gen_data.generate_data(custom_unit_dict, \n",
    "                                                                             has_class=True,\n",
    "                                                                             add_start_end_tag=True)\n",
    "del gen_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T21:24:03.444204Z",
     "start_time": "2017-10-26T21:24:03.330601Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Val data\")\n",
    "print(\"text\",np.array(x_val_22_T).shape)\n",
    "print(\"gene\",np.array(x_val_22_G).shape, x_val_22_G[0])\n",
    "print(\"variation\",np.array(x_val_22_V).shape, x_val_22_V[0])\n",
    "print(\"classes\",np.array(x_val_22_C).shape, x_val_22_C[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T00:32:56.437499Z",
     "start_time": "2017-10-26T00:32:56.413156Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_unknown_tag_idx   = corpus_wordidx[\"<UNK>\"]\n",
    "char_unknown_tag_idx   = global_utils.char_unknown_tag_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T00:32:56.450804Z",
     "start_time": "2017-10-26T00:32:56.439121Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MAX_TEXT_LEN = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T00:32:57.501666Z",
     "start_time": "2017-10-26T00:32:56.452013Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2988, 5000) (333, 5000)\n"
     ]
    }
   ],
   "source": [
    "x_train_22_T = pad_sequences(x_train_22_T, maxlen=MAX_TEXT_LEN, value=word_unknown_tag_idx,\n",
    "                                  padding=\"post\",truncating=\"post\")\n",
    "x_val_22_T = pad_sequences(x_val_22_T, maxlen=MAX_TEXT_LEN, value=word_unknown_tag_idx,\n",
    "                                  padding=\"post\",truncating=\"post\")\n",
    "print(x_train_22_T.shape, x_val_22_T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T00:32:57.536590Z",
     "start_time": "2017-10-26T00:32:57.502761Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2988, 1) (2988, 4)\n",
      "(333, 1) (333, 4)\n"
     ]
    }
   ],
   "source": [
    "MAX_GENE_LEN = 1\n",
    "MAX_VAR_LEN = 4\n",
    "x_train_22_G = pad_sequences(x_train_22_G, maxlen=MAX_GENE_LEN, value=word_unknown_tag_idx)\n",
    "x_train_22_V = pad_sequences(x_train_22_V, maxlen=MAX_VAR_LEN, value=word_unknown_tag_idx)\n",
    "\n",
    "x_val_22_G = pad_sequences(x_val_22_G, maxlen=MAX_GENE_LEN, value=word_unknown_tag_idx)\n",
    "x_val_22_V = pad_sequences(x_val_22_V, maxlen=MAX_VAR_LEN, value=word_unknown_tag_idx)\n",
    "\n",
    "print(x_train_22_G.shape, x_train_22_V.shape)\n",
    "print(x_val_22_G.shape, x_val_22_V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "keras np_utils.to_categorical expects zero index categorical variables\n",
    "\n",
    "https://github.com/fchollet/keras/issues/570"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T00:32:57.542037Z",
     "start_time": "2017-10-26T00:32:57.538132Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_train_22_C = np.array(x_train_22_C) - 1\n",
    "x_val_22_C = np.array(x_val_22_C) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T00:32:57.576726Z",
     "start_time": "2017-10-26T00:32:57.543764Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2988, 9) (333, 9)\n"
     ]
    }
   ],
   "source": [
    "x_train_22_C = np_utils.to_categorical(np.array(x_train_22_C), 9)\n",
    "x_val_22_C = np_utils.to_categorical(np.array(x_val_22_C), 9)\n",
    "print(x_train_22_C.shape, x_val_22_C.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### test Data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-26T03:43:32.887420Z",
     "start_time": "2017-09-26T03:43:29.372697Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gen_data = global_utils.GenerateDataset(test_df, corpus_wordidx)\n",
    "x_test_22_T, x_test_22_G, x_test_22_V, _ = gen_data.generate_data(custom_unit_dict, \n",
    "                                                                has_class=False,\n",
    "                                                                add_start_end_tag=True)\n",
    "del gen_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-26T03:43:33.178763Z",
     "start_time": "2017-09-26T03:43:32.888877Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data\n",
      "text (986,)\n",
      "gene (986, 3) [364606, 188717, 364607]\n",
      "variation (986,) [364606, 317947, 364607]\n"
     ]
    }
   ],
   "source": [
    "print(\"Test data\")\n",
    "print(\"text\",np.array(x_test_22_T).shape)\n",
    "print(\"gene\",np.array(x_test_22_G).shape, x_test_22_G[0])\n",
    "print(\"variation\",np.array(x_test_22_V).shape, x_test_22_V[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-26T03:43:33.546461Z",
     "start_time": "2017-09-26T03:43:33.181689Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(986, 5000)\n"
     ]
    }
   ],
   "source": [
    "x_test_22_T = pad_sequences(x_test_22_T, maxlen=MAX_TEXT_LEN, value=word_unknown_tag_idx,\n",
    "                                  padding=\"post\",truncating=\"post\")\n",
    "print(x_test_22_T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-26T03:43:33.575305Z",
     "start_time": "2017-09-26T03:43:33.548386Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(986, 1) (986, 4)\n"
     ]
    }
   ],
   "source": [
    "MAX_GENE_LEN = 1\n",
    "MAX_VAR_LEN = 4\n",
    "x_test_22_G = pad_sequences(x_test_22_G, maxlen=MAX_GENE_LEN, value=word_unknown_tag_idx)\n",
    "x_test_22_V = pad_sequences(x_test_22_V, maxlen=MAX_VAR_LEN, value=word_unknown_tag_idx)\n",
    "\n",
    "print(x_test_22_G.shape, x_test_22_V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T:text_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T23:07:49.588270Z",
     "start_time": "2017-10-26T23:07:49.582681Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "custom_unit_dict = {\n",
    "         \"gene_unit\"          : \"raw_chars\",\n",
    "         \"variation_unit\"     : \"raw_chars\",\n",
    "         # text transformed to sentences attribute\n",
    "         \"doc_unit\"           : \"raw_chars\",\n",
    "         \"doc_form\"           : \"text\",\n",
    "         \"divide_document\"    : \"multiple_unit\"\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T23:09:42.095904Z",
     "start_time": "2017-10-26T23:07:50.005896Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import global_utils\n",
    "gen_data = global_utils.GenerateDataset(x_train_df, corpus_wordidx)\n",
    "x_train_33_T, x_train_33_G, x_train_33_V, x_train_33_C = gen_data.generate_data(custom_unit_dict, \n",
    "                                                                             has_class=True,\n",
    "                                                                             add_start_end_tag=True)\n",
    "del gen_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T23:09:48.474776Z",
     "start_time": "2017-10-26T23:09:42.097144Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      "text (1086419,) [74, 71, 19, 7, 4, 72, 71, 19, 20, 12, 14, 17, 72, 71, 18, 20, 15, 15, 17, 4, 18, 18, 14, 17, 72, 71, 6, 4, 13, 4, 72, 71, 15, 19, 4, 13, 72, 71, 8, 18, 72, 71, 5, 17, 4, 16, 20, 4, 13, 19, 11, 24, 72, 71, 12, 20, 19, 0, 19, 4, 3, 72, 71, 8, 13, 72, 71, 3, 8, 21, 4, 17, 18, 4, 72, 71, 7, 20, 12, 0, 13, 72, 71, 2, 0, 13, 2, 4, 17, 18, 72, 71, 0, 13, 3, 72, 71, 8, 13, 72, 71, 0, 20, 19, 14, 18, 14, 12, 0, 11, 72, 71, 3, 14, 12, 8, 13, 0, 13, 19, 72, 71, 2, 0, 13, 2, 4, 17, 72, 71, 15, 17, 4, 3, 8, 18, 15, 14, 18, 8, 19, 8, 14, 13, 72, 71, 3, 8, 18, 14, 17, 3, 4, 17, 18, 72, 71, 72, 75]\n",
      "gene (1086419,) [74, 71, 15, 19, 4, 13, 72, 75]\n",
      "variation (1086419,) [74, 71, 24, 27, 32, 2, 72, 75]\n",
      "classes (1086419,) 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Train data\")\n",
    "print(\"text\",np.array(x_train_33_T).shape, x_train_33_T[0])\n",
    "print(\"gene\",np.array(x_train_33_G).shape, x_train_33_G[0])\n",
    "print(\"variation\",np.array(x_train_33_V).shape, x_train_33_V[0])\n",
    "print(\"classes\",np.array(x_train_33_C).shape, x_train_33_C[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T23:10:05.745670Z",
     "start_time": "2017-10-26T23:09:48.475925Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import global_utils\n",
    "gen_data = global_utils.GenerateDataset(x_val_df, corpus_wordidx)\n",
    "x_val_33_T, x_val_33_G, x_val_33_V, x_val_33_C = gen_data.generate_data(custom_unit_dict, \n",
    "                                                                             has_class=True,\n",
    "                                                                             add_start_end_tag=True)\n",
    "del gen_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T23:10:06.510202Z",
     "start_time": "2017-10-26T23:10:05.746898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val data\n",
      "text (128341,) [74, 71, 0, 19, 72, 71, 19, 7, 8, 18, 72, 71, 19, 8, 12, 4, 72, 71, 15, 14, 8, 13, 19, 72, 71, 72, 71, 19, 7, 4, 72, 71, 4, 23, 15, 17, 4, 18, 18, 8, 14, 13, 72, 71, 14, 5, 72, 71, 22, 8, 11, 3, 36, 19, 24, 15, 4, 72, 71, 15, 27, 32, 8, 13, 10, 30, 0, 72, 71, 8, 13, 72, 71, 20, 28, 14, 18, 72, 71, 2, 4, 11, 11, 18, 72, 71, 8, 13, 3, 20, 2, 4, 3, 72, 71, 15, 14, 19, 4, 13, 19, 72, 71, 2, 4, 11, 11, 72, 71, 2, 24, 2, 11, 4, 72, 71, 0, 17, 17, 4, 18, 19, 72, 71, 0, 19, 72, 71, 1, 14, 19, 7, 72, 71, 19, 4, 12, 15, 4, 17, 0, 19, 20, 17, 4, 18, 72, 71, 72, 71, 15, 27, 32, 8, 13, 10, 30, 0, 72, 71, 8, 13, 3, 20, 2, 4, 3, 72, 71, 18, 36, 15, 7, 0, 18, 4, 72, 71, 8, 13, 7, 8, 1, 8, 19, 8, 14, 13, 72, 71, 14, 5, 72, 71, 30, 28, 33, 29, 72, 71, 72, 71, 0, 13, 3, 72, 71, 30, 35, 33, 29, 72, 71, 72, 71, 0, 19, 72, 71, 29, 33, 27, 2, 72, 71, 0, 13, 3, 72, 71, 30, 26, 27, 2, 72, 71, 72, 71, 17, 4, 18, 15, 4, 2, 19, 8, 21, 4, 11, 24, 72, 71, 72, 75]\n",
      "gene (128341,) [74, 71, 2, 3, 10, 13, 28, 0, 72, 75]\n",
      "variation (128341,) [74, 71, 0, 32, 26, 21, 72, 75]\n",
      "classes (128341,) 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Val data\")\n",
    "print(\"text\",np.array(x_val_33_T).shape, x_val_33_T[98])\n",
    "print(\"gene\",np.array(x_val_33_G).shape, x_val_33_G[0])\n",
    "print(\"variation\",np.array(x_val_33_V).shape, x_val_33_V[0])\n",
    "print(\"classes\",np.array(x_val_33_C).shape, x_val_33_C[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T23:10:06.513430Z",
     "start_time": "2017-10-26T23:10:06.511325Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_unknown_tag_idx   = corpus_wordidx[\"<UNK>\"]\n",
    "char_unknown_tag_idx   = global_utils.char_unknown_tag_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T23:10:06.527659Z",
     "start_time": "2017-10-26T23:10:06.514422Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_CHAR_IN_SENT_LEN = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T23:10:18.903599Z",
     "start_time": "2017-10-26T23:10:06.529246Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1086419, 150) (128341, 150)\n"
     ]
    }
   ],
   "source": [
    "x_train_33_T = pad_sequences(x_train_33_T, maxlen=MAX_CHAR_IN_SENT_LEN, value=char_unknown_tag_idx,\n",
    "                                  padding=\"post\",truncating=\"post\")\n",
    "x_val_33_T = pad_sequences(x_val_33_T, maxlen=MAX_CHAR_IN_SENT_LEN, value=char_unknown_tag_idx,\n",
    "                                  padding=\"post\",truncating=\"post\")\n",
    "print(x_train_33_T.shape, x_val_33_T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T23:10:27.876369Z",
     "start_time": "2017-10-26T23:10:18.905017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1086419, 150) (1086419, 150)\n",
      "(128341, 150) (128341, 150)\n"
     ]
    }
   ],
   "source": [
    "x_train_33_G = pad_sequences(x_train_33_G, maxlen=MAX_CHAR_IN_SENT_LEN, value=char_unknown_tag_idx)\n",
    "x_train_33_V = pad_sequences(x_train_33_V, maxlen=MAX_CHAR_IN_SENT_LEN, value=char_unknown_tag_idx)\n",
    "\n",
    "x_val_33_G = pad_sequences(x_val_33_G, maxlen=MAX_CHAR_IN_SENT_LEN, value=char_unknown_tag_idx)\n",
    "x_val_33_V = pad_sequences(x_val_33_V, maxlen=MAX_CHAR_IN_SENT_LEN, value=char_unknown_tag_idx)\n",
    "\n",
    "print(x_train_33_G.shape, x_train_33_V.shape)\n",
    "print(x_val_33_G.shape, x_val_33_V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras np_utils.to_categorical expects zero index categorical variables\n",
    "\n",
    "https://github.com/fchollet/keras/issues/570"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T23:10:27.985411Z",
     "start_time": "2017-10-26T23:10:27.877544Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_33_C = np.array(x_train_33_C) - 1\n",
    "x_val_33_C = np.array(x_val_33_C) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T23:10:28.016098Z",
     "start_time": "2017-10-26T23:10:27.986732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1086419, 9) (128341, 9)\n"
     ]
    }
   ],
   "source": [
    "x_train_33_C = np_utils.to_categorical(np.array(x_train_33_C), 9)\n",
    "x_val_33_C = np_utils.to_categorical(np.array(x_val_33_C), 9)\n",
    "print(x_train_33_C.shape, x_val_33_C.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T:text_sent_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:20:00.685040Z",
     "start_time": "2017-10-27T02:20:00.670683Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "custom_unit_dict = {\n",
    "         \"gene_unit\"          : \"words\",\n",
    "         \"variation_unit\"     : \"words\",\n",
    "         # text transformed to sentences attribute\n",
    "         \"doc_unit\"           : \"word_list\",\n",
    "         \"doc_form\"           : \"text\",\n",
    "         \"divide_document\"    : \"single_unit\"\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:20:15.952551Z",
     "start_time": "2017-10-27T02:20:00.686423Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import global_utils\n",
    "gen_data = global_utils.GenerateDataset(x_train_df, corpus_wordidx)\n",
    "x_train_34_T, x_train_34_G, x_train_34_V, x_train_34_C = gen_data.generate_data(custom_unit_dict, \n",
    "                                                                             has_class=True,\n",
    "                                                                             add_start_end_tag=True)\n",
    "del gen_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:20:17.054217Z",
     "start_time": "2017-10-27T02:20:15.953722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      "text (2988,) [[352216, 252037, 202038, 70974, 86431, 164788, 109857, 338562, 123191, 209585, 221967, 49123, 331220, 140212, 209585, 229015, 140770, 182848, 111721, 8208, 0, 352217]]\n",
      "gene (2988, 3) [352216, 164788, 352217]\n",
      "variation (2988,) [352216, 86196, 352217]\n",
      "classes (2988,) 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Train data\")\n",
    "print(\"text\",np.array(x_train_34_T).shape, x_train_34_T[0][:1])\n",
    "print(\"gene\",np.array(x_train_34_G).shape, x_train_34_G[0])\n",
    "print(\"variation\",np.array(x_train_34_V).shape, x_train_34_V[0])\n",
    "print(\"classes\",np.array(x_train_34_C).shape, x_train_34_C[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:20:18.329383Z",
     "start_time": "2017-10-27T02:20:17.055319Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import global_utils\n",
    "gen_data = global_utils.GenerateDataset(x_val_df, corpus_wordidx)\n",
    "x_val_34_T, x_val_34_G, x_val_34_V, x_val_34_C = gen_data.generate_data(custom_unit_dict, \n",
    "                                                                             has_class=True,\n",
    "                                                                             add_start_end_tag=True)\n",
    "del gen_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:20:18.467889Z",
     "start_time": "2017-10-27T02:20:18.330665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val data\n",
      "text (333,) [[352216, 252037, 156537, 91785, 67201, 109857, 123191, 209585, 213751, 5638, 0, 126280, 49123, 331220, 0, 352217]]\n",
      "gene (333, 3) [352216, 217983, 352217]\n",
      "variation (333,) [352216, 41934, 352217]\n",
      "classes (333,) 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Val data\")\n",
    "print(\"text\",np.array(x_val_34_T).shape, x_val_34_T[98][:1])\n",
    "print(\"gene\",np.array(x_val_34_G).shape, x_val_34_G[0])\n",
    "print(\"variation\",np.array(x_val_34_V).shape, x_val_34_V[0])\n",
    "print(\"classes\",np.array(x_val_34_C).shape, x_val_34_C[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:20:18.479648Z",
     "start_time": "2017-10-27T02:20:18.468992Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_unknown_tag_idx   = corpus_wordidx[\"<UNK>\"]\n",
    "char_unknown_tag_idx   = global_utils.char_unknown_tag_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:20:18.502495Z",
     "start_time": "2017-10-27T02:20:18.481643Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_DOC_LEN = 500 # no of sentences in a document\n",
    "MAX_SENT_LEN = 80 # no of words in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:20:28.645457Z",
     "start_time": "2017-10-27T02:20:18.504233Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for doc_i, doc in enumerate(x_train_34_T):\n",
    "    x_train_34_T[doc_i] = x_train_34_T[doc_i][:MAX_DOC_LEN]\n",
    "    # padding sentences\n",
    "    if len(x_train_34_T[doc_i]) < MAX_DOC_LEN:\n",
    "        for not_used_i in range(0,MAX_DOC_LEN - len(x_train_34_T[doc_i])):\n",
    "            x_train_34_T[doc_i].append([word_unknown_tag_idx]*MAX_SENT_LEN)\n",
    "    # padding words\n",
    "    x_train_34_T[doc_i] = pad_sequences(x_train_34_T[doc_i], maxlen=MAX_SENT_LEN, value=word_unknown_tag_idx)\n",
    "    \n",
    "for doc_i, doc in enumerate(x_val_34_T):\n",
    "    x_val_34_T[doc_i] = x_val_34_T[doc_i][:MAX_DOC_LEN]\n",
    "    # padding sentences\n",
    "    if len(x_val_34_T[doc_i]) < MAX_DOC_LEN:\n",
    "        for not_used_i in range(0,MAX_DOC_LEN - len(x_val_34_T[doc_i])):\n",
    "            x_val_34_T[doc_i].append([word_unknown_tag_idx]*MAX_SENT_LEN)\n",
    "    # padding words\n",
    "    x_val_34_T[doc_i] = pad_sequences(x_val_34_T[doc_i], maxlen=MAX_SENT_LEN, value=word_unknown_tag_idx)\n",
    "    \n",
    "x_train_34_T = np.array(x_train_34_T)\n",
    "x_val_34_T = np.array(x_val_34_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:20:28.650166Z",
     "start_time": "2017-10-27T02:20:28.646889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(333, 500, 80) (2988, 500, 80)\n"
     ]
    }
   ],
   "source": [
    "print(x_val_34_T.shape, x_train_34_T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:20:28.722888Z",
     "start_time": "2017-10-27T02:20:28.651790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2988, 80) (2988, 80)\n",
      "(333, 80) (333, 80)\n"
     ]
    }
   ],
   "source": [
    "x_train_34_G = pad_sequences(x_train_34_G, maxlen=MAX_SENT_LEN, value=word_unknown_tag_idx)\n",
    "x_train_34_V = pad_sequences(x_train_34_V, maxlen=MAX_SENT_LEN, value=word_unknown_tag_idx)\n",
    "\n",
    "x_val_34_G = pad_sequences(x_val_34_G, maxlen=MAX_SENT_LEN, value=word_unknown_tag_idx)\n",
    "x_val_34_V = pad_sequences(x_val_34_V, maxlen=MAX_SENT_LEN, value=word_unknown_tag_idx)\n",
    "\n",
    "print(x_train_34_G.shape, x_train_34_V.shape)\n",
    "print(x_val_34_G.shape, x_val_34_V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras np_utils.to_categorical expects zero index categorical variables\n",
    "\n",
    "https://github.com/fchollet/keras/issues/570"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:20:28.728851Z",
     "start_time": "2017-10-27T02:20:28.724555Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_34_C = np.array(x_train_34_C) - 1\n",
    "x_val_34_C = np.array(x_val_34_C) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:20:28.742924Z",
     "start_time": "2017-10-27T02:20:28.730480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2988, 9) (333, 9)\n"
     ]
    }
   ],
   "source": [
    "x_train_34_C = np_utils.to_categorical(np.array(x_train_34_C), 9)\n",
    "x_val_34_C = np_utils.to_categorical(np.array(x_val_34_C), 9)\n",
    "print(x_train_34_C.shape, x_val_34_C.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Need to form 3 dimensional target data for rationale model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:20:28.795678Z",
     "start_time": "2017-10-27T02:20:28.744106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2988, 500, 9) (333, 500, 9)\n"
     ]
    }
   ],
   "source": [
    "temp = (x_train_34_C.shape[0],1,x_train_34_C.shape[1])\n",
    "x_train_34_C_sent = np.repeat(x_train_34_C.reshape(temp[0],temp[1],temp[2]), MAX_DOC_LEN, axis=1)\n",
    "\n",
    "#sentence test targets\n",
    "temp = (x_val_34_C.shape[0],1,x_val_34_C.shape[1])\n",
    "x_val_34_C_sent = np.repeat(x_val_34_C.reshape(temp[0],temp[1],temp[2]), MAX_DOC_LEN, axis=1)\n",
    "\n",
    "print(x_train_34_C_sent.shape, x_val_34_C_sent.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:20:28.818264Z",
     "start_time": "2017-10-27T02:20:28.796758Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORD_EMB_SIZE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:20:52.297548Z",
     "start_time": "2017-10-27T02:20:28.819450Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(352220, 200)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%autoreload\n",
    "import global_utils\n",
    "ft_file_path = \"/home/bicepjai/Projects/Deep-Survey-Text-Classification/data_prep/processed/stage1/pretrained_word_vectors/ft_sg_200d_50e.vec\"\n",
    "trained_embeddings = global_utils.get_embeddings_from_ft(ft_file_path, WORD_EMB_SIZE, corpus_vocab_list)\n",
    "trained_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### for characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T23:10:28.019172Z",
     "start_time": "2017-10-26T23:10:28.017168Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "CHAR_EMB_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-26T23:10:28.051353Z",
     "start_time": "2017-10-26T23:10:28.020527Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 64)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_embeddings = np.random.randn(global_utils.CHAR_ALPHABETS_LEN, CHAR_EMB_SIZE)\n",
    "char_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:20:52.329224Z",
     "start_time": "2017-10-27T02:20:52.298667Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import tensorflow.contrib.keras as keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.engine import Layer, InputSpec, InputLayer\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "from keras.layers import Dropout, Embedding, concatenate\n",
    "from keras.layers import Conv1D, MaxPooling1D, Conv2D, MaxPooling2D, ZeroPadding1D\n",
    "from keras.layers import Dense, Input, Flatten, BatchNormalization\n",
    "from keras.layers import Concatenate, Dot, Merge, Multiply, RepeatVector\n",
    "from keras.layers import Bidirectional, TimeDistributed\n",
    "from keras.layers import SimpleRNN, LSTM, GRU, Lambda, Permute\n",
    "\n",
    "from keras.layers.core import Reshape, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping,TensorBoard\n",
    "from keras.constraints import maxnorm\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T06:58:17.661183Z",
     "start_time": "2017-08-24T06:58:17.655020Z"
    }
   },
   "source": [
    "## model_1: paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T04:27:34.478402Z",
     "start_time": "2017-09-19T04:27:34.475759Z"
    }
   },
   "source": [
    "refer https://github.com/bwallace/rationale-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Doc-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T01:40:27.506592Z",
     "start_time": "2017-10-27T01:40:27.145419Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "doc_input = Input(shape=(MAX_DOC_LEN,MAX_SENT_LEN,), dtype=\"int16\")\n",
    "reshape_1d = Reshape([MAX_DOC_LEN * MAX_SENT_LEN])(doc_input)\n",
    "doc_embedding_1d = Embedding(vocab_size, WORD_EMB_SIZE, weights=[trained_embeddings], trainable=True)(reshape_1d)\n",
    "# data_format='channels_first' for conv2d\n",
    "doc_embedding = Reshape([1, MAX_DOC_LEN, MAX_SENT_LEN * WORD_EMB_SIZE])(doc_embedding_1d)\n",
    "\n",
    "sent_convs_in_doc = []\n",
    "ngram_filters = [2,3]\n",
    "n_filters = 32\n",
    "\n",
    "# using Conv2D instead of Conv1D since we need to deal with sentences and not the whole document\n",
    "# All Input shape: 4D tensor with shape: (samples, channels, rows, cols) if data_format='channels_first'\n",
    "for n_gram in ngram_filters:\n",
    "    l_conv = Conv2D(filters = n_filters, \n",
    "                    kernel_size = (1, n_gram * WORD_EMB_SIZE), # n_gram words\n",
    "                    strides = (1, WORD_EMB_SIZE), # one word\n",
    "                    data_format='channels_first',\n",
    "                    activation=\"relu\")(doc_embedding)\n",
    "    # this output (n_filters x max_doc_len x 1)\n",
    "    l_pool = MaxPooling2D(pool_size=(1, (MAX_SENT_LEN - n_gram + 1)),\n",
    "                       data_format='channels_first')(l_conv)\n",
    "    \n",
    "    # flip around, to get (1 x DOC_SEQ_LEN x n_filters)\n",
    "    permuted = Permute((2,1,3)) (l_pool)\n",
    "    \n",
    "    # drop extra dimension\n",
    "    reshaped = Reshape((MAX_DOC_LEN, n_filters))(permuted)\n",
    "    sent_convs_in_doc.append(reshaped)\n",
    "    \n",
    "l_concat = Concatenate(axis=1)(sent_convs_in_doc)\n",
    "l_dropout = Dropout(0.5)(l_concat)\n",
    "                       \n",
    "def sum_sent_vecs(x):\n",
    "    return K.sum(x, axis=1)\n",
    "\n",
    "def sum_sent_vec_output_shape(input_shape): \n",
    "    # should be (batch x MAX_DOC_LEN x MAX_SENT_LEN)\n",
    "    shape = list(input_shape) \n",
    "    # something like (None, 96), where 96 is the\n",
    "    # length of induced sentence vectors\n",
    "    return (shape[0], shape[-1])\n",
    "\n",
    "doc_vector = Lambda(sum_sent_vecs, \n",
    "                    output_shape=sum_sent_vec_output_shape)(l_dropout)\n",
    "l_softmax = Dense(9, activation='softmax')(doc_vector)\n",
    "doc_cnn_model = Model(inputs=[doc_input], outputs=[l_softmax])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T01:40:30.149066Z",
     "start_time": "2017-10-27T01:40:30.088356Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 500, 80)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)              (None, 40000)         0           input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 40000, 200)    70444000    reshape_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)              (None, 1, 500, 16000) 0           embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 32, 500, 79)   12832       reshape_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 32, 500, 78)   19232       reshape_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)   (None, 32, 500, 1)    0           conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)   (None, 32, 500, 1)    0           conv2d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "permute_1 (Permute)              (None, 500, 32, 1)    0           max_pooling2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "permute_2 (Permute)              (None, 500, 32, 1)    0           max_pooling2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)              (None, 500, 32)       0           permute_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)              (None, 500, 32)       0           permute_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 1000, 32)      0           reshape_5[0][0]                  \n",
      "                                                                   reshape_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 1000, 32)      0           concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (None, 32)            0           dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 9)             297         lambda_1[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 70,476,361\n",
      "Trainable params: 70,476,361\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "doc_cnn_model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['categorical_accuracy'])\n",
    "doc_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T01:40:40.355534Z",
     "start_time": "2017-10-27T01:40:40.020776Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%rm -rf ./tb_graphs/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T01:40:41.068872Z",
     "start_time": "2017-10-27T01:40:41.062981Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tb_callback = keras.callbacks.TensorBoard(log_dir='./tb_graphs', histogram_freq=0, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T01:40:42.016111Z",
     "start_time": "2017-10-27T01:40:42.010465Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=\"doc_cnn_weights.hdf5\", \n",
    "                                    verbose=1,\n",
    "                                    monitor=\"val_categorical_accuracy\",\n",
    "                                    save_best_only=True,\n",
    "                                    mode=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T01:45:03.891005Z",
     "start_time": "2017-10-27T01:41:17.252373Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no checkpoints available !\n",
      "Train on 2988 samples, validate on 333 samples\n",
      "Epoch 1/5\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 9.1418 - categorical_accuracy: 0.2853Epoch 00000: val_categorical_accuracy improved from -inf to 0.37838, saving model to doc_cnn_weights.hdf5\n",
      "2988/2988 [==============================] - 51s - loss: 9.1449 - categorical_accuracy: 0.2855 - val_loss: 8.5094 - val_categorical_accuracy: 0.3784\n",
      "Epoch 2/5\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 5.4533 - categorical_accuracy: 0.4385Epoch 00001: val_categorical_accuracy improved from 0.37838 to 0.56156, saving model to doc_cnn_weights.hdf5\n",
      "2988/2988 [==============================] - 44s - loss: 5.4415 - categorical_accuracy: 0.4391 - val_loss: 1.6307 - val_categorical_accuracy: 0.5616\n",
      "Epoch 3/5\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 1.1139 - categorical_accuracy: 0.6546Epoch 00002: val_categorical_accuracy improved from 0.56156 to 0.61862, saving model to doc_cnn_weights.hdf5\n",
      "2988/2988 [==============================] - 44s - loss: 1.1131 - categorical_accuracy: 0.6550 - val_loss: 1.2385 - val_categorical_accuracy: 0.6186\n",
      "Epoch 4/5\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 0.7613 - categorical_accuracy: 0.7433Epoch 00003: val_categorical_accuracy improved from 0.61862 to 0.63363, saving model to doc_cnn_weights.hdf5\n",
      "2988/2988 [==============================] - 44s - loss: 0.7621 - categorical_accuracy: 0.7430 - val_loss: 1.4087 - val_categorical_accuracy: 0.6336\n",
      "Epoch 5/5\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 0.6487 - categorical_accuracy: 0.7782Epoch 00004: val_categorical_accuracy did not improve\n",
      "2988/2988 [==============================] - 42s - loss: 0.6513 - categorical_accuracy: 0.7771 - val_loss: 1.4816 - val_categorical_accuracy: 0.6306\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # model = keras.models.load_model('current_model.h5')\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    try:\n",
    "        doc_cnn_model.load_weights(\"model_weights.hdf5\")\n",
    "    except IOError as ioe:\n",
    "        print(\"no checkpoints available !\")\n",
    "    doc_cnn_model.fit(x_train_34_T, x_train_34_C, \n",
    "          validation_data=(x_val_34_T, x_val_34_C),\n",
    "          epochs=5, batch_size=32, shuffle=True,\n",
    "          callbacks=[tb_callback,checkpointer])\n",
    "    #model.save('current_sent_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RA-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:20:53.074657Z",
     "start_time": "2017-10-27T02:20:52.330807Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_input = Input(shape=(MAX_DOC_LEN,MAX_SENT_LEN,), dtype=\"int32\")\n",
    "reshape_1d = Reshape([MAX_DOC_LEN * MAX_SENT_LEN])(doc_input)\n",
    "doc_embedding_1d = Embedding(vocab_size, WORD_EMB_SIZE, weights=[trained_embeddings], trainable=True)(reshape_1d)\n",
    "# data_format='channels_first' for conv2d\n",
    "doc_embedding = Reshape([1, MAX_DOC_LEN, MAX_SENT_LEN * WORD_EMB_SIZE])(doc_embedding_1d)\n",
    "\n",
    "sent_convs_in_doc = []\n",
    "ngram_filters = [2,3]\n",
    "n_filters = 32 # nof features\n",
    "final_doc_dims = len(ngram_filters) * n_filters \n",
    "\n",
    "# using Conv2D instead of Conv1D since we need to deal with sentences and not the whole document\n",
    "# All Input shape: 4D tensor with shape: (samples, channels, rows, cols) if data_format='channels_first'\n",
    "for n_gram in ngram_filters:\n",
    "    l_conv = Conv2D(filters = n_filters, \n",
    "                    kernel_size = (1, n_gram * WORD_EMB_SIZE), # n_gram words\n",
    "                    strides = (1, WORD_EMB_SIZE), # one word\n",
    "                    data_format='channels_first',\n",
    "                    activation=\"relu\")(doc_embedding)\n",
    "    # this output (n_filters x max_doc_len x 1)\n",
    "    l_pool = MaxPooling2D(pool_size=(1, (MAX_SENT_LEN - n_gram + 1)),\n",
    "                       data_format='channels_first')(l_conv)\n",
    "    \n",
    "    # flip around, to get (1 x DOC_SEQ_LEN x n_filters)\n",
    "    permuted = Permute((2,1,3)) (l_pool)\n",
    "    \n",
    "    # drop extra dimension\n",
    "    reshaped = Reshape((MAX_DOC_LEN, n_filters))(permuted)\n",
    "    sent_convs_in_doc.append(reshaped)\n",
    "    \n",
    "sent_vectors = concatenate(sent_convs_in_doc)\n",
    "# do we need dropout here, we might lose information\n",
    "# l_dropout = Dropout(0.5)(l_concat)\n",
    "\n",
    "sentence_softmax = Dense(9, activation='softmax', kernel_regularizer=l2(0.01), name=\"sentence_prediction\")\n",
    "doc_sent_output_layer = TimeDistributed(sentence_softmax, name=\"sentence_predictions\")(sent_vectors)\n",
    "\n",
    "# weights are set to the estimated probabilities that \n",
    "# corresponding sentences are rationales in the most likely direction\n",
    "sum_weighting_probs = Lambda(lambda x: K.max(x, axis=1))\n",
    "\n",
    "# distributing over sentences in the document\n",
    "sent_weights = TimeDistributed(sum_weighting_probs)(doc_sent_output_layer)\n",
    "\n",
    "# reshaping the weights to perform matrix dot product\n",
    "reshaped_sent_weights = Reshape((1, MAX_DOC_LEN))(sent_weights)\n",
    "\n",
    "# along the last 2 axes and not including the batch axis\n",
    "doc_vector = Dot((1,2))([sent_vectors, reshaped_sent_weights])\n",
    "doc_vector = Reshape((final_doc_dims,))(doc_vector)\n",
    "\n",
    "l_dropout = Dropout(0.5)(doc_vector)\n",
    "doc_output_layer = Dense(9, activation=\"softmax\")(l_dropout)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:20:53.082232Z",
     "start_time": "2017-10-27T02:20:53.076014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 500, 80)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)              (None, 40000)         0           input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 40000, 200)    70444000    reshape_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)              (None, 1, 500, 16000) 0           embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 32, 500, 79)   12832       reshape_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 32, 500, 78)   19232       reshape_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)   (None, 32, 500, 1)    0           conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)   (None, 32, 500, 1)    0           conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "permute_1 (Permute)              (None, 500, 32, 1)    0           max_pooling2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "permute_2 (Permute)              (None, 500, 32, 1)    0           max_pooling2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)              (None, 500, 32)       0           permute_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)              (None, 500, 32)       0           permute_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 500, 64)       0           reshape_3[0][0]                  \n",
      "                                                                   reshape_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "sentence_predictions (TimeDistri (None, 500, 9)        585         concatenate_1[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 70,476,649\n",
      "Trainable params: 70,476,649\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentence_model = Model(inputs=doc_input, outputs=doc_sent_output_layer)\n",
    "sentence_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:20:53.101197Z",
     "start_time": "2017-10-27T02:20:53.083508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 500, 80)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)              (None, 40000)         0           input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 40000, 200)    70444000    reshape_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)              (None, 1, 500, 16000) 0           embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 32, 500, 79)   12832       reshape_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 32, 500, 78)   19232       reshape_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)   (None, 32, 500, 1)    0           conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)   (None, 32, 500, 1)    0           conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "permute_1 (Permute)              (None, 500, 32, 1)    0           max_pooling2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "permute_2 (Permute)              (None, 500, 32, 1)    0           max_pooling2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)              (None, 500, 32)       0           permute_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)              (None, 500, 32)       0           permute_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 500, 64)       0           reshape_3[0][0]                  \n",
      "                                                                   reshape_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "sentence_predictions (TimeDistri (None, 500, 9)        585         concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistribu (None, 500)           0           sentence_predictions[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)              (None, 1, 500)        0           time_distributed_1[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "dot_1 (Dot)                      (None, 64, 1)         0           concatenate_1[0][0]              \n",
      "                                                                   reshape_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)              (None, 64)            0           dot_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 64)            0           reshape_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 9)             585         dropout_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 70,477,234\n",
      "Trainable params: 70,477,234\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ra_cnn_model = Model(inputs=doc_input, outputs=doc_output_layer)\n",
    "ra_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training sentence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T01:58:54.540025Z",
     "start_time": "2017-10-27T01:58:54.498372Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the accuracy is not improving anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T01:58:54.811855Z",
     "start_time": "2017-10-27T01:58:54.541528Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%rm -rf ./tb_graphs/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T01:58:54.816392Z",
     "start_time": "2017-10-27T01:58:54.813632Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb_callback = keras.callbacks.TensorBoard(log_dir='./tb_graphs', histogram_freq=0, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T01:58:54.843069Z",
     "start_time": "2017-10-27T01:58:54.817770Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=\"sentence_model_weights.hdf5\", \n",
    "                                    verbose=1,\n",
    "                                    monitor=\"val_categorical_accuracy\",\n",
    "                                    save_best_only=True,\n",
    "                                    mode=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:06:42.833189Z",
     "start_time": "2017-10-27T01:59:19.535701Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no checkpoints available !\n",
      "Train on 2988 samples, validate on 333 samples\n",
      "Epoch 1/10\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 2.0019 - categorical_accuracy: 0.2940Epoch 00000: val_categorical_accuracy improved from -inf to 0.34185, saving model to sentence_model_weights.hdf5\n",
      "2988/2988 [==============================] - 50s - loss: 2.0012 - categorical_accuracy: 0.2941 - val_loss: 1.8185 - val_categorical_accuracy: 0.3418\n",
      "Epoch 2/10\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 1.7393 - categorical_accuracy: 0.3599Epoch 00001: val_categorical_accuracy improved from 0.34185 to 0.38441, saving model to sentence_model_weights.hdf5\n",
      "2988/2988 [==============================] - 43s - loss: 1.7395 - categorical_accuracy: 0.3595 - val_loss: 1.6787 - val_categorical_accuracy: 0.3844\n",
      "Epoch 3/10\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 1.6239 - categorical_accuracy: 0.4055Epoch 00002: val_categorical_accuracy improved from 0.38441 to 0.40451, saving model to sentence_model_weights.hdf5\n",
      "2988/2988 [==============================] - 43s - loss: 1.6239 - categorical_accuracy: 0.4057 - val_loss: 1.6334 - val_categorical_accuracy: 0.4045\n",
      "Epoch 4/10\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 1.5615 - categorical_accuracy: 0.4266Epoch 00003: val_categorical_accuracy improved from 0.40451 to 0.40813, saving model to sentence_model_weights.hdf5\n",
      "2988/2988 [==============================] - 43s - loss: 1.5608 - categorical_accuracy: 0.4265 - val_loss: 1.6237 - val_categorical_accuracy: 0.4081\n",
      "Epoch 5/10\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 1.5066 - categorical_accuracy: 0.4456Epoch 00004: val_categorical_accuracy improved from 0.40813 to 0.42128, saving model to sentence_model_weights.hdf5\n",
      "2988/2988 [==============================] - 43s - loss: 1.5064 - categorical_accuracy: 0.4453 - val_loss: 1.5871 - val_categorical_accuracy: 0.4213\n",
      "Epoch 6/10\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 1.4613 - categorical_accuracy: 0.4568Epoch 00005: val_categorical_accuracy improved from 0.42128 to 0.42535, saving model to sentence_model_weights.hdf5\n",
      "2988/2988 [==============================] - 43s - loss: 1.4608 - categorical_accuracy: 0.4575 - val_loss: 1.5852 - val_categorical_accuracy: 0.4253\n",
      "Epoch 7/10\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 1.4241 - categorical_accuracy: 0.4729Epoch 00006: val_categorical_accuracy improved from 0.42535 to 0.42715, saving model to sentence_model_weights.hdf5\n",
      "2988/2988 [==============================] - 43s - loss: 1.4235 - categorical_accuracy: 0.4727 - val_loss: 1.5885 - val_categorical_accuracy: 0.4271\n",
      "Epoch 8/10\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 1.3939 - categorical_accuracy: 0.4822Epoch 00007: val_categorical_accuracy improved from 0.42715 to 0.42794, saving model to sentence_model_weights.hdf5\n",
      "2988/2988 [==============================] - 43s - loss: 1.3932 - categorical_accuracy: 0.4821 - val_loss: 1.5954 - val_categorical_accuracy: 0.4279\n",
      "Epoch 9/10\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 1.3696 - categorical_accuracy: 0.4809Epoch 00008: val_categorical_accuracy improved from 0.42794 to 0.43104, saving model to sentence_model_weights.hdf5\n",
      "2988/2988 [==============================] - 43s - loss: 1.3690 - categorical_accuracy: 0.4807 - val_loss: 1.5869 - val_categorical_accuracy: 0.4310\n",
      "Epoch 10/10\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 1.3480 - categorical_accuracy: 0.4916Epoch 00009: val_categorical_accuracy improved from 0.43104 to 0.43368, saving model to sentence_model_weights.hdf5\n",
      "2988/2988 [==============================] - 43s - loss: 1.3491 - categorical_accuracy: 0.4914 - val_loss: 1.5879 - val_categorical_accuracy: 0.4337\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # model = keras.models.load_model('current_model.h5')\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    try:\n",
    "        sentence_model.load_weights(\"sentence_model_weights.hdf5\")\n",
    "    except IOError as ioe:\n",
    "        print(\"no checkpoints available !\")\n",
    "    sentence_model.fit(x_train_34_T, x_train_34_C_sent, \n",
    "          validation_data=(x_val_34_T, x_val_34_C_sent),\n",
    "          epochs=10, batch_size=32, shuffle=True,\n",
    "          callbacks=[tb_callback,checkpointer])\n",
    "    #sentence_model.save('current_sent_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:14:07.771356Z",
     "start_time": "2017-10-27T02:07:04.889020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2988 samples, validate on 333 samples\n",
      "Epoch 1/10\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 1.3352 - categorical_accuracy: 0.4924Epoch 00000: val_categorical_accuracy did not improve\n",
      "2988/2988 [==============================] - 41s - loss: 1.3344 - categorical_accuracy: 0.4926 - val_loss: 1.6008 - val_categorical_accuracy: 0.4330\n",
      "Epoch 2/10\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 1.3199 - categorical_accuracy: 0.4996Epoch 00001: val_categorical_accuracy did not improve\n",
      "2988/2988 [==============================] - 41s - loss: 1.3209 - categorical_accuracy: 0.4993 - val_loss: 1.6259 - val_categorical_accuracy: 0.4326\n",
      "Epoch 3/10\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 1.3099 - categorical_accuracy: 0.5022Epoch 00002: val_categorical_accuracy did not improve\n",
      "2988/2988 [==============================] - 41s - loss: 1.3105 - categorical_accuracy: 0.5021 - val_loss: 1.6049 - val_categorical_accuracy: 0.4299\n",
      "Epoch 4/10\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 1.2968 - categorical_accuracy: 0.4990Epoch 00003: val_categorical_accuracy did not improve\n",
      "2988/2988 [==============================] - 41s - loss: 1.2967 - categorical_accuracy: 0.4992 - val_loss: 1.6209 - val_categorical_accuracy: 0.4263\n",
      "Epoch 5/10\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 1.2876 - categorical_accuracy: 0.5025Epoch 00004: val_categorical_accuracy improved from 0.43368 to 0.43606, saving model to sentence_model_weights.hdf5\n",
      "2988/2988 [==============================] - 43s - loss: 1.2885 - categorical_accuracy: 0.5020 - val_loss: 1.6221 - val_categorical_accuracy: 0.4361\n",
      "Epoch 6/10\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 1.2840 - categorical_accuracy: 0.5087Epoch 00005: val_categorical_accuracy did not improve\n",
      "2988/2988 [==============================] - 41s - loss: 1.2840 - categorical_accuracy: 0.5087 - val_loss: 1.6442 - val_categorical_accuracy: 0.4261\n",
      "Epoch 7/10\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 1.2762 - categorical_accuracy: 0.4984Epoch 00006: val_categorical_accuracy improved from 0.43606 to 0.43756, saving model to sentence_model_weights.hdf5\n",
      "2988/2988 [==============================] - 43s - loss: 1.2765 - categorical_accuracy: 0.4982 - val_loss: 1.6294 - val_categorical_accuracy: 0.4376\n",
      "Epoch 8/10\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 1.2674 - categorical_accuracy: 0.5108Epoch 00007: val_categorical_accuracy did not improve\n",
      "2988/2988 [==============================] - 41s - loss: 1.2680 - categorical_accuracy: 0.5104 - val_loss: 1.6453 - val_categorical_accuracy: 0.4365\n",
      "Epoch 9/10\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 1.2625 - categorical_accuracy: 0.5107Epoch 00008: val_categorical_accuracy did not improve\n",
      "2988/2988 [==============================] - 41s - loss: 1.2633 - categorical_accuracy: 0.5102 - val_loss: 1.6495 - val_categorical_accuracy: 0.4295\n",
      "Epoch 10/10\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 1.2599 - categorical_accuracy: 0.5079Epoch 00009: val_categorical_accuracy improved from 0.43756 to 0.43798, saving model to sentence_model_weights.hdf5\n",
      "2988/2988 [==============================] - 43s - loss: 1.2600 - categorical_accuracy: 0.5078 - val_loss: 1.6586 - val_categorical_accuracy: 0.4380\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # model = keras.models.load_model('current_model.h5')\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    try:\n",
    "        sentence_model.load_weights(\"sentence_model_weights.hdf5\")\n",
    "    except IOError as ioe:\n",
    "        print(\"no checkpoints available !\")\n",
    "    sentence_model.fit(x_train_34_T, x_train_34_C_sent, \n",
    "          validation_data=(x_val_34_T, x_val_34_C_sent),\n",
    "          epochs=10, batch_size=32, shuffle=True,\n",
    "          callbacks=[tb_callback,checkpointer])\n",
    "    #sentence_model.save('current_sent_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### training ra_cnn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are freezing the sentence level trained model which will be used as input for rationale model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:20:53.261442Z",
     "start_time": "2017-10-27T02:20:53.102380Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_model.load_weights(\"sentence_model_weights.hdf5\")\n",
    "sent_softmax_layer = ra_cnn_model.get_layer(\"sentence_predictions\")\n",
    "sent_softmax_layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:20:53.285135Z",
     "start_time": "2017-10-27T02:20:53.262650Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ra_cnn_model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:20:53.550199Z",
     "start_time": "2017-10-27T02:20:53.286340Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%rm -rf ./tb_graphs/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:20:53.554862Z",
     "start_time": "2017-10-27T02:20:53.552007Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb_callback = keras.callbacks.TensorBoard(log_dir='./tb_graphs', histogram_freq=0, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:20:53.588262Z",
     "start_time": "2017-10-27T02:20:53.556157Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=\"ra_cnn_model_weights.hdf5\", \n",
    "                                    verbose=1,\n",
    "                                    monitor=\"val_categorical_accuracy\",\n",
    "                                    save_best_only=True,\n",
    "                                    mode=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T02:28:38.037258Z",
     "start_time": "2017-10-27T02:21:25.239907Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no checkpoints available !\n",
      "Train on 2988 samples, validate on 333 samples\n",
      "Epoch 1/10\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 1.8309 - categorical_accuracy: 0.4052Epoch 00000: val_categorical_accuracy improved from -inf to 0.57057, saving model to ra_cnn_model_weights.hdf5\n",
      "2988/2988 [==============================] - 50s - loss: 1.8302 - categorical_accuracy: 0.4053 - val_loss: 1.3053 - val_categorical_accuracy: 0.5706\n",
      "Epoch 2/10\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 1.2696 - categorical_accuracy: 0.5985Epoch 00001: val_categorical_accuracy improved from 0.57057 to 0.62162, saving model to ra_cnn_model_weights.hdf5\n",
      "2988/2988 [==============================] - 43s - loss: 1.2717 - categorical_accuracy: 0.5971 - val_loss: 1.1158 - val_categorical_accuracy: 0.6216\n",
      "Epoch 3/10\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 0.9867 - categorical_accuracy: 0.6838Epoch 00002: val_categorical_accuracy improved from 0.62162 to 0.65766, saving model to ra_cnn_model_weights.hdf5\n",
      "2988/2988 [==============================] - 43s - loss: 0.9866 - categorical_accuracy: 0.6837 - val_loss: 1.0573 - val_categorical_accuracy: 0.6577\n",
      "Epoch 4/10\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 0.7690 - categorical_accuracy: 0.7476Epoch 00003: val_categorical_accuracy did not improve\n",
      "2988/2988 [==============================] - 41s - loss: 0.7692 - categorical_accuracy: 0.7473 - val_loss: 1.1015 - val_categorical_accuracy: 0.6396\n",
      "Epoch 5/10\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 0.6559 - categorical_accuracy: 0.7728Epoch 00004: val_categorical_accuracy did not improve\n",
      "2988/2988 [==============================] - 41s - loss: 0.6558 - categorical_accuracy: 0.7728 - val_loss: 1.1975 - val_categorical_accuracy: 0.6517\n",
      "Epoch 6/10\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 0.5853 - categorical_accuracy: 0.7886Epoch 00005: val_categorical_accuracy improved from 0.65766 to 0.65766, saving model to ra_cnn_model_weights.hdf5\n",
      "2988/2988 [==============================] - 43s - loss: 0.5855 - categorical_accuracy: 0.7882 - val_loss: 1.2630 - val_categorical_accuracy: 0.6577\n",
      "Epoch 7/10\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 0.5211 - categorical_accuracy: 0.8075Epoch 00006: val_categorical_accuracy did not improve\n",
      "2988/2988 [==============================] - 41s - loss: 0.5213 - categorical_accuracy: 0.8079 - val_loss: 1.3363 - val_categorical_accuracy: 0.6366\n",
      "Epoch 8/10\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 0.4887 - categorical_accuracy: 0.8263Epoch 00007: val_categorical_accuracy did not improve\n",
      "2988/2988 [==============================] - 41s - loss: 0.4886 - categorical_accuracy: 0.8263 - val_loss: 1.4293 - val_categorical_accuracy: 0.6306\n",
      "Epoch 9/10\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 0.4748 - categorical_accuracy: 0.8256Epoch 00008: val_categorical_accuracy did not improve\n",
      "2988/2988 [==============================] - 41s - loss: 0.4744 - categorical_accuracy: 0.8256 - val_loss: 1.5342 - val_categorical_accuracy: 0.6426\n",
      "Epoch 10/10\n",
      "2976/2988 [============================>.] - ETA: 0s - loss: 0.4517 - categorical_accuracy: 0.8290Epoch 00009: val_categorical_accuracy did not improve\n",
      "2988/2988 [==============================] - 41s - loss: 0.4511 - categorical_accuracy: 0.8290 - val_loss: 1.5798 - val_categorical_accuracy: 0.6156\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # ra_cnn_model = keras.models.load_model('current_model.h5')\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    try:\n",
    "        ra_cnn_model.load_weights(\"ra_cnn_model_weights.hdf5\")\n",
    "    except IOError as ioe:\n",
    "        print(\"no checkpoints available !\")\n",
    "    ra_cnn_model.fit(x_train_34_T, x_train_34_C, \n",
    "          validation_data=(x_val_34_T, x_val_34_C),\n",
    "          epochs=10, batch_size=32, shuffle=True,\n",
    "          callbacks=[tb_callback,checkpointer])\n",
    "    #ra_cnn_model.save('current_sent_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "833px",
    "left": "0px",
    "right": "1192px",
    "top": "52px",
    "width": "300px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
